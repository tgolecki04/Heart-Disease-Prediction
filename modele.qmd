---
title: "Wczesne Wykrywanie Ryzyka Zawału Serca"
subtitle: "Modele Predykcyjne"
format: html
editor: visual
---

## Wstęp

Modele predykcyjne stanowią kluczowy element analityki danych i sztucznej inteligencji. Pozwalają przewidywać przyszłe zdarzenia, klasyfikować obserwacje oraz identyfikować ukryte zależności w danych. W praktyce biznesowej i naukowej stosuje się wiele podejść modelowania – od klasycznych metod statystycznych, przez algorytmy uczenia maszynowego, aż po zaawansowane architektury sieci neuronowych.\

Na tej podstronie przedstawiamy trzy popularne i często stosowane techniki modelowania predykcyjnego: **regresję logistyczną**, **las losowy** oraz **sieci neuronowe (NN)**. Każda z nich opiera się na innym mechanizmie działania i oferuje różne możliwości modelowania złożoności danych.

W rozważanym zbiorze danych posiadamy zmienną predykcyjną `Ten_Year_CHD`, która jest zmienną binarną - przyjmuje wartości 0 lub 1. Wartość 0 oznacza brak zagrożenia wystąpienia choroby wieńcowej w ciągu 10 najbliższych lat, natomiast zmienna przyjmująca wartość 1, oznacza ryzyko wystąpienia choroby. Struktura naszych danych mówi jasno w jaki sposób powinniśmy tworzyć nasze modele. Zajmiemy się problemem klasyfikacji, starając się każdemu z pacjentów określić grupę do której należy - potencjalne ryzyko lub jego brak. Dodatkowo tworząc model oparty o architekturę sieci neuronowej rozszerzymy zakres naszej klasyfikacji z binarnej na wieloczynnikową. Zanim omówimy każdy z modeli osobno, chcemy poruszyć kwestię uczenia maszynowego oraz pojęć z tym związanym w celu lepszego przedstawienia problemu oraz ułatwienia zrozumienia architektury stworzonych modeli. Modele te zostały stworzone w języku programowania `python` przy użyciu bilbiotek takich jak:

-   Scikit-learn

-   Pandas

-   Numpy

-   Pytorch

## Machine Learning

### Wyjaśnienie

**Machine Learning (ML), czyli uczenie maszynowe**, to dziedzina sztucznej inteligencji, która umożliwia komputerom **uczenie się na podstawie danych** i podejmowanie decyzji bez konieczności ręcznego programowania każdej reguły.\

Model ML analizuje dostarczone mu przykłady, identyfikuje wzorce oraz uczy się przewidywać przyszłe zdarzenia, klasyfikować obiekty czy podejmować optymalne decyzje.

### Struktura

![](images/article/picture_4.png){fig-align="center" width="683"}

### Poddziedziny

#### Uczenie nadzorowane (Supervised Learning)

##### Wyjaśnienie

Model uczy się na danych, dla których znamy prawidłową odpowiedź (tzw. etykietę).\
Celem jest przewidywanie wyników dla nowych, nieznanych danych.

##### Przykłady

-   Klasyfikacja (np. wykrywanie spamu, diagnoza chorób),

-   Regresja (np. prognoza cen, przewidywanie popytu).

#### Uczenie nienadzorowane (Unsupervised Learning)

##### Wyjaśnienie

Model szuka struktury w danych bez etykiet. Celem jest odkrywanie ukrytych zależności, grup lub reprezentacji danych.

##### Przykłady

-   Klasteryzacja

-   Redukcja Wymiarów

#### Uczenie półnadzorowane (Semi-Supervised Learning)

##### Wyjaśnienie

Połączenie uczenia nadzorowanego i nienadzorowanego – model uczy się na niewielkiej ilości danych z etykietami i dużej ilości danych bez nich.

#### Uczenie ze wzmocnieniem (Reinforcement Learning)

##### Wyjaśnienie

Agent uczy się poprzez interakcję z otoczeniem, otrzymując nagrody lub kary.\

Celem jest opracowanie strategii maksymalizującej długoterminową nagrodę.

##### Przykłady

-   Gry

-   Sterowanie robotami

-   Autonomiczne pojazdy

#### Uczenie głębokie (Deep Learning)

Model wykorzystuje sieci składające się z wielu warstw przetwarzających dane. Dzięki takiemu zastosowaniu jesteśmy w stanie znajdować głębokie wzorce i zależności, które nie są możliwe do zauważenia stosując pozostałe architektury modeli. Sieć nazywamy "głęboką" ponieważ ma wiele poziomów. Wyróżniamy:

-   warstwę wejściową

-   wiele warstw ukrytych

-   warstwę wyjściową

Każda warstwa przetwarza dane w inny sposób co pozwala na połączenie skomplikowanych obliczeń i wykrycie ważnych oraz skomplikowanych zależności.

##### Przykłady

**CNN (Convolutional Neural Networks)** – do obrazów, wideo, sygnałów.

**RNN (Recurrent Neural Networks)**, **LSTM**, **GRU** – do sekwencji, tekstu, sygnałów czasowych.

**Transformers** – obecny standard w NLP, analizie języka, obrazu i wielu innych dziedzinach.

![](images/svg/ml_map.svg){fig-align="center" width="700"}

### Typy zadań

Uczenie maszynowe możemy podzielić na **paradygmaty** (np. supervised, unsupervised), które określają sposób, w jaki model uczy się informacji z danych. Omawiając te paradygmaty wyróżniamy **różne typy zadań**, czyli określone problemy, które chcemy rozwiązać za pomocą algorytmów uczenia maszynowego. Każde zadanie ma własny, zdefiniowany cel i wymaga użycia odpowiednich metod i algorytmów.

#### Klasyfikacja

-   Cel

    -   Celem klasyfikacji jest przypisanie obserwacji do określonej grupy / kategorii.

-   Algorytmy

    -   Decision Trees & Random Forest

    -   Support Vector Machines (SVM)

    -   k-Nearest Neighbors

    -   Sieci Neuronowe (NN)

    -   Regresja Logistyczna

-   Przykłady zastosowań

    -   Rozpoznawanie spamu w wiadomościach e-mail (spam/nie-spam)

    -   Diagnoza chorób na podstawie danych medycznych

    -   Klasyfikacja zdjęć pojazdów

#### Regresja

-   Cel

    -   Estymacja wartości liczbowych na podstawie danych

-   Algorytmy

    -   Regresja Liniowa

    -   Random Forest Regression

    -   Support Vector Regression

    -   Sieci Neuronowe (NN)

-   Przykłady zastosowań

    -   Prognozowanie cen

    -   Przewidywanie wartości opadów, temperatury, itp.

    -   Szacowanie przychodu / wydatków

#### Klasteryzacja

-   Cel

    -   Grupowanie obserwacji na podstawie cech oraz ich podobieństwa, bez wcześniejszego określenia etykiet i grup w danych.

-   Algorytmy

    -   K-means

    -   DBSCAN

-   Przykłady zastosowań

    -   Grupowanie klientów sklepu internetowego

    -   Segmentacja dokumentów

    -   Wykrywanie nowych zależności w danych

#### Redukcja Wymiarowości

-   Cel

    -   Zmniejszenie liczby zmiennych przy zachowaniu jak największej ilości informacji w danych. Taki zabieg ułatwia wizualizację oraz przyspiesza proces uczenia.

-   Algorytmy

    -   Principal Component Analysis (PCA)

    -   Independent Component Analysis (ICA)

    -   Factor Analysis (Analiza Czynnikowa)

    -   Uniform Manifold Approximation and Projection (UMAP)

    -   t-Distributed Stochastic Neighbor Embedding (t-SNE)

-   Przykłady zastosowań

    -   Wizualizacja danych wielowymiarowych w dwóch lub trzech wymiarach

    -   Przygotowanie danych do przeprowadzenia dalszej analizy (np. klasteryzacji lub klasyfikacji)

### Ocena Modeli

Ocena modeli uczenia maszynowego polega na sprawdzeniu, **jak dobrze model wykonuje swoje zadanie** na danych, których wcześniej nigdy nie widział. Prawidłowa ocena pozwala wykryć przeuczenie (overfitting), niedouczenie (underfitting), porównać różne algorytmy oraz wybrać najlepszy model do wdrożenia.

Sposób oceny modelu zależy od typu wykonywanego zadania, każdy rodzaj problemów ocenia się inny, odpowiedni dla danego rozwiązania sposób, ale ogólny proces oceny modelu opiera się na kilku wspólnych elementach.

#### Podział Danych

#### Metryki

#### Wykresy Diagnostyczne

#### Walidacja Krzyżowa

## Rozwiązanie problemu

### Regresja Logistyczna

Regresja logistyczna to jeden z najprostszych i najczęściej używanych modeli uczenia maszynowego do **klasyfikacji binarnej.** Jest to **model klasyfikacyjny**, który przewiduje **prawdopodobieństwo przynależności do jednej z dwóch klas.** Wykorzystuje on funkcję **sigmoidalną (logistyczną),** której zadaniem jest przeskalowanie wyników do zakresu 0-1, czyli do postaci prawdopodobieństwa.

Funkcja ta ma postać:

$$
\sigma(z) =\frac{1}{1+e^{-z}}
$$

Następnie, po otrzymaniu wartości prawdopodobieństwa, każdej obserwacji przypisujemy odpowiednią klasę przy użyciu progu odcięcia. Przyjmując próg równy 0.5, mamy:

-   jeśli $P\ge 0.5 \rightarrow klasa~1$

-   jeśli $P\ge 0.5 \rightarrow klasa~0$

Warto pamiętać, że wybór wartości progu odcięcia zależy od konkretnego zastosowania i powinien być ustalany na podstawie biznesowych metryk

::: {.alert .alert-dismissible .alert-danger}
<button type="button" class="btn-close" data-bs-dismiss="alert"></button> <strong>Uwaga!</strong> Dobór wartości progu wpływa na jakość metryk, takich jak recall lub precision.
:::

Chcąc zwiększyć dokładność modelu, możemy zoptymalizować wykorzystywane parametry. W przypadku regresji liniowej w celu znalezienia optymalnych parametrów wykorzystujemy metodę najmniejszych kwadratów. Ze względu na strukturę regresji logistycznej analogiczna metoda byłaby błędna, ponieważ regresja logistyczna przewiduje prawdopodobieństwo przynależności do określonej grupy, a nie szacuje wartość parametru. W takiej sytuacji wykorzystujemy metodę największej wiarygodności (**Maximum likelihood - MLE**). MLE szuka wartości parametrów, które maksymalizują funkcję wiarygodności, czyli takich, dla których obserwowane dane są najbardziej prawdopodobne. Algorytm MLE używa gradientu i kroku uczenia optymalizując funkcję wiarygodności poprzez iteracyjne aktualizowanie wartości parametrów w kierunku maksimum funkcji.

Gradient to wektor nachylenia funkcji, który mówi w jaki sposób i o jaką wartość zmienić każdą wagę modelu aby zmiejszyć błąd.

Learning rate to **krok, o jaki przesuwamy parametry w dół gradientu**, odpowiada za to jak szybko wykonujemy zmiany wag w kierunku wskazanym przez gradient.

### Random Forest & Drzewo Decyzyjne

#### Drzewo Decyzyjne

Drzewo decyzyjne to model, który polega na podejmowaniu kolejnych decyzji i podziale danych na mniejsze grupy. Taki proces powtarzamy wielokrotnie tworząc rozbudowaną strukturę stworzoną z warunków logicznych `if/else`. W zależności od odpowiedzi, poruszamy się w dół drzewa przez odpowiednie węzły, aż ostatecznie trafimy na **liść**, który oznacza:

-   etykietę klasy w przypadku klasyfikacji

-   estymowaną wartość w przypadku regresji

Model szuka podziału, który **najlepiej porządkuje dane**, czyli minimalizuje niepewność związaną z danymi. Do najczęstszych miar podziału zaliczamy:

-   Gini impurity (klasyfikacja)

-   Entropia

-   MSE (regresja)

#### Las Losowy

Las losowy jest to **duża kolekcja drzew decyzyjnych**, które uczone są w sposób losowy, a ich przewidywania są **połączone**. Takie podejście poprawia jakość podstawowego elementu składowego, jakim jest drzewo decyzyjne, ponieważ jedno drzewo może się mylić, natomiast wykorzystanie wielu drzew i wybranie predykcjina podstawie większej próby znacznie ulepsza jakość modelu oraz daje dokładniejszą predykcję.

### Sieci Neuronowe

## Podsumowanie

<div>

![](images/article/picture_2.png){style="margin-top: 25px;" fig-align="center"}

</div>
